{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <p style=\"color:Blue;\">Missing Data Imputation</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Numerical Variables\n",
    "#### Mean / Median Imputation: If the variable is normally distributed the mean and median are approximately the same, If the variable is skewed, the median is a better representation. When to useData is missing completely at random, No more than 5% of the variable contains missing data.\n",
    "#### Arbitrary value imputation: Need to be careful not to chose an arbitrary value too similar to the mean or median (or any other common value of the variable distribution), Arbitrary value imputation consists of replacing all occurrences of missing values (NA) within a variable by an arbitrary value, Typically used arbitrary values are 0, 999, -999 (or other combinations of 9s) or -1 (if the distribution is positive).\n",
    "#### End of tail imputation: End of tail imputation is equivalent to arbitrary value imputation, but automatically selecting arbitrary values at the end of the variable distributions. If the variable is normally distributed, we can use the mean plus or minus 3 times the standard deviation. If the variable is skewed, we can use the IQR proximity rule. Suitable numerical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Categorical Variables\n",
    "#### Frequent category imputation: Mode imputation consists of replacing all occurrences of missing values (NA) within a variable by the mode, or the most frequent value.\n",
    "#### Adding a “missing” category: This method consists in treating missing data as an additional label or category of the variable. This is the most widely used method of missing data imputation for categorical variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Both\n",
    "#### Random sample imputation: Random sampling consist in taking a random observation from the pool of available observations of the variable, and using that randomly extracted value to fill the NA. \n",
    "\n",
    "Extract 3 random elements from the df, Note that we use random_state\n",
    "to ensure the reproducibility of the example: <br>\n",
    "df[\"abc\"].sample(n=3, random_state=1)\n",
    "\n",
    "#### Complete Case Analysis\n",
    "#### Adding a “Missing” indicator: A Missing Indicator is an additional binary variable, which indicates whether the data was missing for an observation (1) or not (0).\n",
    "\n",
    "X_train['ABC_MI'] = np.where(X_train['abc'].isnull(), 1, 0)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### When to use a missing indicator:\n",
    "Typically, mean, median and mode imputation are done together with adding a binary \"missing indicator\" variable to capture those observations where the data was missing (see lecture \"Missing Indicator\"), thus covering 2 angles:\n",
    "if the data was missing completely at random, this would be captured by the mean, median or mode imputation, and if it wasn't this would be captured by the additional \"missing indicator\" variable.\n",
    "Both methods are extremely straight forward to implement, and therefore are a top choice in data science competitions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### SimpleImputer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Imputation transformer for completing missing values:\n",
    "SimpleImputer(missing_values=nan, strategy='mean', fill_value=None, verbose=0, copy=True, add_indicator=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "imputer = SimpleImputer(strategy='median')\n",
    "imputer.fit(X_train[cols_to_use])\n",
    "# NOTE: the data is returned as a numpy array!!!\n",
    "X_train = imputer.transform(X_train)\n",
    "X_train = pd.DataFrame(X_train, columns=cols_to_use)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ColumnTransformer\n",
    "\n",
    "Applies transformers to columns of an array or pandas DataFrame.\n",
    "\n",
    "This estimator allows different columns or column subsets of the input to be transformed separately and the features generated by each transformer will be concatenated to form a single feature space. This is useful for heterogeneous or columnar data, to combine several feature extraction mechanisms or transformations into a single transformer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# first we need to make lists, indicating which features\n",
    "# will be imputed with each method\n",
    "\n",
    "features_numeric = ['BsmtUnfSF', 'LotFrontage', 'MasVnrArea', ]\n",
    "features_categoric = ['BsmtQual', 'FireplaceQu', 'MSZoning',\n",
    "                      'Street', 'Alley']\n",
    "\n",
    "# then we instantiate the imputers, within a pipeline\n",
    "# we create one mean imputer and one frequent category imputer\n",
    "# by changing the parameter in the strategy\n",
    "\n",
    "numeric_imputer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='mean')),\n",
    "])\n",
    "\n",
    "categoric_imputer = Pipeline(steps=[\n",
    "    ('imputer', SimpleImputer(strategy='most_frequent')),\n",
    "])\n",
    "\n",
    "# then we put the features list and the transformers together\n",
    "# using the column transformer\n",
    "\n",
    "preprocessor = ColumnTransformer(transformers=[\n",
    "    ('numeric_imputer', numeric_imputer, features_numeric),\n",
    "    ('categoric_imputer', categoric_imputer, features_categoric)\n",
    "])\n",
    "\n",
    "# now we fit the preprocessor\n",
    "preprocessor.fit(X_train)\n",
    "\n",
    "# and now we can impute the data\n",
    "X_train = preprocessor.transform(X_train)\n",
    "X_test = preprocessor.transform(X_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Add a Missing Indicator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "indicator = MissingIndicator(error_on_new=True, features='missing-only')\n",
    "indicator.fit(X_train)  \n",
    "tmp = indicator.transform(X_train)\n",
    "\n",
    "# so we need to join it manually to the original X_train\n",
    "\n",
    "# let's create a column name for each of the new MissingIndicators\n",
    "indicator_cols = [c+'_NA' for c in X_train.columns[indicator.features_]]\n",
    "\n",
    "# and now we concatenate\n",
    "X_train = pd.concat([\n",
    "    X_train.reset_index(),\n",
    "    pd.DataFrame(tmp, columns = indicator_cols)],\n",
    "    axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
